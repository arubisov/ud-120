{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from helper import featureFormatDF, run_logistic_regression_classifier, run_decision_tree_classifier, plot_roc\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "financial_features = ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees']\n",
    "#financial_features = ['salary', 'deferral_payments', 'total_payments', 'bonus', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock']\n",
    "email_features = ['to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros: 160 nans: 1358 pois: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3066"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = 0\n",
    "nans = 0\n",
    "pois = 0\n",
    "for k, v in data_dict.iteritems():\n",
    "    for a, b in v.iteritems():\n",
    "        if b == 'NaN':\n",
    "            nans += 1\n",
    "        if b == 0:\n",
    "            zeros += 1\n",
    "        if a == 'poi' and b == True:\n",
    "            pois += 1\n",
    "\n",
    "print \"zeros: {} nans: {} pois: {}\".format(zeros, nans, pois)\n",
    "\n",
    "146*21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "for k,v in data_dict.items():\n",
    "    if v['bonus'] != 'NaN' and v['bonus'] != 'NaN':\n",
    "        data_dict[k]['bonus_to_salary_ratio'] = 1.0 * v['bonus'] / v['salary']\n",
    "    else:\n",
    "        data_dict[k]['bonus_to_salary_ratio'] = 'NaN' \n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "### Extract features and labels from dataset for local testing\n",
    "features_list = ['poi'] + ['bonus_to_salary_ratio'] + financial_features + email_features\n",
    "# features_list = ['poi'] + email_features\n",
    "df = featureFormatDF(my_dataset, features_list, sort_keys = True)\n",
    "df = df.drop(['TOTAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for col in df.columns:\n",
    "#     sparse = sum(df[col] == 0)\n",
    "#     if sparse > 100:\n",
    "#         print col, sparse\n",
    "    \n",
    "# print sum(df.poi * df.director_fees != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a0816bc10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/kAAAHrCAYAAACUxF21AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0816b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "plt.rcParams['figure.figsize'] = (15,8)\n",
    "\n",
    "df.boxplot(features_list[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "df.shape\n",
    "print(sum(df.poi == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Check which rows have mostly zero values, which salaries are missing...\n",
    "# # turns out it's most, so this was not useful.\n",
    "# df.apply(lambda x: sum(x == 0), axis=1)\n",
    "# df[df.salary == 0]\n",
    "# sum(df.poi == 1)\n",
    "\n",
    "# # Automatic outlier removal by standard deviation\n",
    "# # https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-dataframe\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "# df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "# df.shape\n",
    "# sum(df.poi == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# conda install -c conda-forge imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, fbeta_score, make_scorer\n",
    "\n",
    "def run_logistic_regression_classifier(X_train, X_test, y_train, y_test, beta=1):\n",
    "        \n",
    "    #X_resampled, y_resampled = SMOTE().fit_sample(X_train, y_train)\n",
    "    #X_train = X_resampled\n",
    "    #y_train = y_resampled\n",
    "    \n",
    "    print \"{} ({:.2f}%) positive labels in training set with resampling\".format(len(y_train[y_train == True]), 100.0*len(y_train[y_train == True])/len(y_train))\n",
    "    print \"{} ({:.2f}%) positive labels in test set\".format(len(y_test[y_test == True]), 100.0*len(y_test[y_test == True])/len(y_test))\n",
    "    \n",
    "    t0 = time()\n",
    "    param_grid = {\n",
    "             'C': [1e1, 1e2, 1e3, 1e4, 1e5, 1e6],\n",
    "              'penalty': ['l1', 'l2'],\n",
    "              }\n",
    "    f5_scorer = make_scorer(fbeta_score, beta=beta)\n",
    "    clf = GridSearchCV(estimator=LogisticRegression(class_weight='balanced'),\n",
    "                       param_grid=param_grid,\n",
    "                       scoring=f5_scorer)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    print \"training time: {}s\".format(round(time() - t0, 3))\n",
    "    print \"Best estimator found by grid search:\"\n",
    "    print clf.best_estimator_\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print \"{} ({:.2f}%) positive labels in predictions\".format(len(y_pred[y_pred == True]), 100.0*len(y_pred[y_pred == True])/len(y_pred))\n",
    "\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    print conf\n",
    "    TN, FP, FN, TP = conf.ravel()\n",
    "    print \"TN: {}, FP: {}, FN: {}, TP: {}\".format(TN, FP, FN, TP)\n",
    "\n",
    "    # accuracy: what proportion was correctly predicted out of total population?\n",
    "    print \"accuracy: {}\".format((TP+TN)*1.0/(TN+FP+FN+TP))\n",
    "    # precision: of all the ones you predicted to be of class X, what ratio was correct?\n",
    "    print \"precision: {}\".format(TP*1.0/(TP+FP))\n",
    "    # recall: of all the ones that ARE of class X, what ratio did you successfully predict?\n",
    "    print \"recall: {}\".format(TP*1.0/(TP+FN))\n",
    "    \n",
    "    return clf.best_estimator_\n",
    "\n",
    "\n",
    "def plot_roc(clf, X_test, y_test, X_train=None, y_train=None):\n",
    "    y_score = clf.decision_function(X_test)\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='Test ROC curve (area = %0.2f)' % roc_auc)\n",
    "    \n",
    "    if X_train is not None and y_train is not None:\n",
    "        y_score_train = clf.decision_function(X_train)\n",
    "        \n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr_train, tpr_train, _ = roc_curve(y_train, y_score_train)\n",
    "        roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "        plt.plot(fpr_train, tpr_train, color='aqua',\n",
    "                 lw=lw, label='Train ROC curve (area = %0.2f)' % roc_auc_train)\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC for Logistic Regression Model')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: SMOTE resampling\n",
      "13 (13.00%) positive labels in training set\n",
      "5 (11.36%) positive labels in test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonrubisov/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'std_train_score': array([ 0.        ,  0.05366275,  0.09382107,  0.03292806,  0.03813215,\n",
      "        0.05540361,  0.02452813,  0.09456486,  0.00708879,  0.03796601,\n",
      "        0.        ,  0.0867379 ,  0.10686374,  0.04970437,  0.0269374 ,\n",
      "        0.09069008,  0.06521976,  0.08885158,  0.06521976,  0.07496569,\n",
      "        0.        ,  0.0638021 ,  0.09382107,  0.0399324 ,  0.03183832,\n",
      "        0.04652039,  0.07443229,  0.08391004,  0.07443229,  0.09921499,\n",
      "        0.        ,  0.0638021 ,  0.07969445,  0.03600411,  0.03183832,\n",
      "        0.07101721,  0.07443229,  0.0689609 ,  0.07443229,  0.04161637,\n",
      "        0.        ,  0.06106632,  0.07479068,  0.04571649,  0.03218231,\n",
      "        0.07523537,  0.07443229,  0.05899157,  0.07443229,  0.07523537,\n",
      "        0.        ,  0.06106632,  0.07479068,  0.03600411,  0.01604308,\n",
      "        0.05624023,  0.0942809 ,  0.07494854,  0.0942809 ,  0.07976159]), 'rank_test_score': array([55, 32, 23, 21,  5, 44,  2, 17,  1, 44, 55, 32, 23, 22, 27, 25, 20,\n",
      "        8,  3, 17, 55, 43, 32, 46,  4, 26, 13,  9, 13, 10, 55, 32, 36, 46,\n",
      "        6, 41, 13, 41, 13, 30, 55, 36, 36, 51,  7, 30, 11, 46, 11, 28, 55,\n",
      "       36, 36, 51, 29, 46, 53, 46, 53, 17], dtype=int32), 'split1_train_score': array([ 0.        ,  0.42857143,  0.40909091,  0.5625    ,  0.75      ,\n",
      "        0.6       ,  0.85714286,  0.5625    ,  0.85714286,  0.6       ,\n",
      "        0.        ,  0.42857143,  0.40909091,  0.5625    ,  0.85714286,\n",
      "        0.5       ,  0.9       ,  0.54545455,  0.9       ,  0.54545455,\n",
      "        0.        ,  0.42857143,  0.40909091,  0.58064516,  0.7826087 ,\n",
      "        0.66666667,  1.        ,  0.5625    ,  1.        ,  0.5625    ,\n",
      "        0.        ,  0.42857143,  0.41860465,  0.6       ,  0.7826087 ,\n",
      "        0.69230769,  1.        ,  0.64285714,  1.        ,  0.6       ,\n",
      "        0.        ,  0.42857143,  0.42857143,  0.64285714,  0.7826087 ,\n",
      "        0.64285714,  1.        ,  0.69230769,  1.        ,  0.64285714,\n",
      "        0.        ,  0.42857143,  0.42857143,  0.6       ,  0.81818182,\n",
      "        0.66666667,  1.        ,  0.6       ,  1.        ,  0.5625    ]), 'split2_train_score': array([ 0.        ,  0.42105263,  0.41025641,  0.64285714,  0.7826087 ,\n",
      "        0.69230769,  0.9       ,  0.75      ,  0.85714286,  0.69230769,\n",
      "        0.        ,  0.42105263,  0.41025641,  0.66666667,  0.85714286,\n",
      "        0.72      ,  1.        ,  0.75      ,  1.        ,  0.72      ,\n",
      "        0.        ,  0.42105263,  0.41025641,  0.66666667,  0.85714286,\n",
      "        0.69230769,  1.        ,  0.75      ,  1.        ,  0.7826087 ,\n",
      "        0.        ,  0.42105263,  0.41025641,  0.66666667,  0.85714286,\n",
      "        0.7826087 ,  1.        ,  0.75      ,  1.        ,  0.69230769,\n",
      "        0.        ,  0.43243243,  0.42105263,  0.66666667,  0.85714286,\n",
      "        0.7826087 ,  1.        ,  0.72      ,  1.        ,  0.7826087 ,\n",
      "        0.        ,  0.43243243,  0.42105263,  0.66666667,  0.85714286,\n",
      "        0.72      ,  1.        ,  0.75      ,  1.        ,  0.75      ]), 'std_score_time': array([  9.61657575e-04,   1.46216629e-04,   1.39634923e-04,\n",
      "         3.73551185e-05,   4.45101556e-04,   1.03340879e-04,\n",
      "         6.98628367e-05,   4.30618247e-05,   7.99113732e-05,\n",
      "         1.84784226e-05,   9.85867792e-05,   1.05900322e-04,\n",
      "         2.16624881e-04,   1.06603236e-04,   8.17592277e-05,\n",
      "         8.76605456e-05,   6.64348426e-05,   6.83229058e-05,\n",
      "         2.74106582e-04,   1.86412669e-04,   7.81858663e-05,\n",
      "         5.58045496e-05,   2.99343702e-04,   1.76347393e-05,\n",
      "         5.55514721e-05,   4.29093098e-05,   1.25427410e-04,\n",
      "         7.19042749e-06,   1.32906947e-04,   1.49810030e-05,\n",
      "         3.89884794e-04,   1.99186733e-04,   1.19413572e-04,\n",
      "         9.06012211e-05,   1.12915860e-04,   6.52451366e-05,\n",
      "         1.67112358e-05,   3.49131066e-05,   7.67402542e-05,\n",
      "         5.16843725e-05,   4.71097791e-05,   3.64438082e-04,\n",
      "         9.18602443e-05,   2.72440103e-05,   5.65955021e-05,\n",
      "         3.43444830e-04,   3.61319317e-05,   4.94906027e-06,\n",
      "         6.68129045e-05,   1.50082597e-04,   3.11900348e-04,\n",
      "         2.57126329e-05,   5.23444478e-05,   1.18938775e-05,\n",
      "         5.55569292e-05,   7.78979171e-05,   2.49286891e-04,\n",
      "         6.83559921e-05,   3.36738051e-05,   5.93070975e-05]), 'split2_test_score': array([ 0.        ,  0.18181818,  0.18181818,  0.26666667,  0.36363636,\n",
      "        0.26666667,  0.15384615,  0.26666667,  0.30769231,  0.26666667,\n",
      "        0.        ,  0.18181818,  0.18181818,  0.26666667,  0.36363636,\n",
      "        0.28571429,  0.15384615,  0.28571429,  0.14285714,  0.26666667,\n",
      "        0.        ,  0.18181818,  0.18181818,  0.26666667,  0.33333333,\n",
      "        0.4       ,  0.16666667,  0.28571429,  0.16666667,  0.30769231,\n",
      "        0.        ,  0.18181818,  0.18181818,  0.26666667,  0.28571429,\n",
      "        0.28571429,  0.16666667,  0.28571429,  0.16666667,  0.28571429,\n",
      "        0.        ,  0.18181818,  0.18181818,  0.25      ,  0.28571429,\n",
      "        0.28571429,  0.15384615,  0.26666667,  0.15384615,  0.30769231,\n",
      "        0.        ,  0.18181818,  0.18181818,  0.25      ,  0.28571429,\n",
      "        0.26666667,  0.15384615,  0.26666667,  0.15384615,  0.26666667]), 'param_lr__C': masked_array(data = [1e-10 1e-10 1.0000000000000001e-05 1.0000000000000001e-05 1.0 1.0 100000.0\n",
      " 100000.0 10000000000.0 10000000000.0 1e-10 1e-10 1.0000000000000001e-05\n",
      " 1.0000000000000001e-05 1.0 1.0 100000.0 100000.0 10000000000.0\n",
      " 10000000000.0 1e-10 1e-10 1.0000000000000001e-05 1.0000000000000001e-05\n",
      " 1.0 1.0 100000.0 100000.0 10000000000.0 10000000000.0 1e-10 1e-10\n",
      " 1.0000000000000001e-05 1.0000000000000001e-05 1.0 1.0 100000.0 100000.0\n",
      " 10000000000.0 10000000000.0 1e-10 1e-10 1.0000000000000001e-05\n",
      " 1.0000000000000001e-05 1.0 1.0 100000.0 100000.0 10000000000.0\n",
      " 10000000000.0 1e-10 1e-10 1.0000000000000001e-05 1.0000000000000001e-05\n",
      " 1.0 1.0 100000.0 100000.0 10000000000.0 10000000000.0],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'mean_score_time': array([ 0.00140063,  0.00103474,  0.00102965,  0.00073401,  0.00107733,\n",
      "        0.00169293,  0.00077065,  0.00059907,  0.00072002,  0.00056259,\n",
      "        0.00065112,  0.00068704,  0.00074061,  0.00068498,  0.00084488,\n",
      "        0.00067099,  0.00082898,  0.00061989,  0.00087595,  0.00068839,\n",
      "        0.00061234,  0.00058508,  0.00081102,  0.00056092,  0.00063006,\n",
      "        0.00057928,  0.00072869,  0.000549  ,  0.00073854,  0.00056728,\n",
      "        0.00087969,  0.00076262,  0.00066741,  0.00065931,  0.00063928,\n",
      "        0.00059772,  0.00070604,  0.00059501,  0.00061973,  0.000585  ,\n",
      "        0.0006729 ,  0.00082405,  0.00067743,  0.00058333,  0.00062799,\n",
      "        0.00083407,  0.00063793,  0.00058071,  0.00068728,  0.00080172,\n",
      "        0.00084591,  0.00062235,  0.00060503,  0.00056601,  0.00064905,\n",
      "        0.00069507,  0.00081706,  0.00062958,  0.00058762,  0.00060972]), 'mean_fit_time': array([ 0.00226235,  0.00365758,  0.00659641,  0.0027717 ,  0.02691897,\n",
      "        0.00550938,  0.01964744,  0.00231131,  0.01708857,  0.0019838 ,\n",
      "        0.00193954,  0.00210285,  0.00557176,  0.00260965,  0.01844772,\n",
      "        0.00299001,  0.01868375,  0.00265066,  0.02325344,  0.00232466,\n",
      "        0.00142638,  0.00160154,  0.00537038,  0.00199604,  0.02371534,\n",
      "        0.0020264 ,  0.01504127,  0.00208163,  0.01500209,  0.00208004,\n",
      "        0.00201416,  0.00222071,  0.00207138,  0.00230138,  0.00922171,\n",
      "        0.00273363,  0.00773501,  0.00244761,  0.00898592,  0.00244935,\n",
      "        0.00151841,  0.00178607,  0.00456858,  0.00237568,  0.01131129,\n",
      "        0.00285172,  0.01093833,  0.00231258,  0.00870466,  0.00306996,\n",
      "        0.00231377,  0.00191339,  0.00430862,  0.00242194,  0.01248272,\n",
      "        0.00342393,  0.00467126,  0.00249497,  0.0052073 ,  0.00246398]), 'split0_train_score': array([ 0.        ,  0.53846154,  0.60869565,  0.60869565,  0.84210526,\n",
      "        0.56      ,  0.84210526,  0.53846154,  0.84210526,  0.63636364,\n",
      "        0.        ,  0.60869565,  0.63636364,  0.56      ,  0.8       ,\n",
      "        0.58333333,  0.84210526,  0.58333333,  0.84210526,  0.58333333,\n",
      "        0.        ,  0.56      ,  0.60869565,  0.58333333,  0.8       ,\n",
      "        0.58333333,  0.84210526,  0.58333333,  0.84210526,  0.58333333,\n",
      "        0.        ,  0.56      ,  0.58333333,  0.58333333,  0.8       ,\n",
      "        0.60869565,  0.84210526,  0.58333333,  0.84210526,  0.60869565,\n",
      "        0.        ,  0.56      ,  0.58333333,  0.56      ,  0.84210526,\n",
      "        0.60869565,  0.84210526,  0.58333333,  0.84210526,  0.60869565,\n",
      "        0.        ,  0.56      ,  0.58333333,  0.58333333,  0.84210526,\n",
      "        0.58333333,  0.8       ,  0.58333333,  0.8       ,  0.60869565]), 'param_kbest__k': masked_array(data = [15 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17\n",
      " 17 17 17 17 17 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 19 19 19 19 19\n",
      " 20 20 20 20 20 20 20 20 20 20],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'std_test_score': array([ 0.        ,  0.02938723,  0.0568365 ,  0.05840672,  0.09354883,\n",
      "        0.10841442,  0.13664301,  0.05664215,  0.11902592,  0.10841442,\n",
      "        0.        ,  0.02938723,  0.0568365 ,  0.05854919,  0.14830293,\n",
      "        0.07366042,  0.04521276,  0.05926954,  0.13926066,  0.05664215,\n",
      "        0.        ,  0.03046808,  0.02938723,  0.10832051,  0.07405138,\n",
      "        0.1655214 ,  0.04256811,  0.06537058,  0.04256811,  0.07987526,\n",
      "        0.        ,  0.02938723,  0.03147048,  0.10832051,  0.05468232,\n",
      "        0.11614535,  0.04256811,  0.11614535,  0.04256811,  0.11617438,\n",
      "        0.        ,  0.03147048,  0.03147048,  0.10162718,  0.05283374,\n",
      "        0.11617438,  0.04247882,  0.10832051,  0.04247882,  0.12498521,\n",
      "        0.        ,  0.03147048,  0.03147048,  0.10162718,  0.11660445,\n",
      "        0.10832051,  0.07234042,  0.10832051,  0.07234042,  0.05664215]), 'mean_train_score': array([ 0.        ,  0.4626952 ,  0.47601432,  0.60468427,  0.79157132,\n",
      "        0.6174359 ,  0.86641604,  0.61698718,  0.85213033,  0.64289044,\n",
      "        0.        ,  0.48610657,  0.48523699,  0.59638889,  0.83809524,\n",
      "        0.60111111,  0.91403509,  0.62626263,  0.91403509,  0.61626263,\n",
      "        0.        ,  0.46987469,  0.47601432,  0.61021505,  0.81325052,\n",
      "        0.6474359 ,  0.94736842,  0.63194444,  0.94736842,  0.64281401,\n",
      "        0.        ,  0.46987469,  0.47073146,  0.61666667,  0.81325052,\n",
      "        0.69453735,  0.94736842,  0.65873016,  0.94736842,  0.63366778,\n",
      "        0.        ,  0.47366795,  0.47765246,  0.6231746 ,  0.82728561,\n",
      "        0.67805383,  0.94736842,  0.66521368,  0.94736842,  0.67805383,\n",
      "        0.        ,  0.47366795,  0.47765246,  0.61666667,  0.83914331,\n",
      "        0.65666667,  0.93333333,  0.64444444,  0.93333333,  0.64039855]), 'split0_test_score': array([ 0.        ,  0.13333333,  0.25      ,  0.15384615,  0.14285714,\n",
      "        0.14285714,  0.15384615,  0.13333333,  0.15384615,  0.14285714,\n",
      "        0.        ,  0.13333333,  0.25      ,  0.13333333,  0.15384615,\n",
      "        0.13333333,  0.15384615,  0.15384615,  0.15384615,  0.13333333,\n",
      "        0.        ,  0.125     ,  0.13333333,  0.13333333,  0.15384615,\n",
      "        0.13333333,  0.15384615,  0.13333333,  0.15384615,  0.13333333,\n",
      "        0.        ,  0.13333333,  0.13333333,  0.13333333,  0.15384615,\n",
      "        0.13333333,  0.15384615,  0.13333333,  0.15384615,  0.15384615,\n",
      "        0.        ,  0.13333333,  0.13333333,  0.13333333,  0.16666667,\n",
      "        0.15384615,  0.16666667,  0.13333333,  0.16666667,  0.15384615,\n",
      "        0.        ,  0.13333333,  0.13333333,  0.13333333,  0.16666667,\n",
      "        0.13333333,  0.15384615,  0.13333333,  0.15384615,  0.13333333]), 'mean_test_score': array([ 0.        ,  0.142     ,  0.18166667,  0.18430769,  0.23457143,\n",
      "        0.13657143,  0.24974359,  0.18833333,  0.30051282,  0.13657143,\n",
      "        0.        ,  0.142     ,  0.18166667,  0.18410256,  0.17230769,\n",
      "        0.18086905,  0.18557692,  0.20159341,  0.24611722,  0.18833333,\n",
      "        0.        ,  0.13916667,  0.142     ,  0.13333333,  0.23564103,\n",
      "        0.17733333,  0.18980769,  0.19461905,  0.18980769,  0.19401465,\n",
      "        0.        ,  0.142     ,  0.14007018,  0.13333333,  0.21259341,\n",
      "        0.13961905,  0.18980769,  0.13961905,  0.18980769,  0.14659341,\n",
      "        0.        ,  0.14007018,  0.14007018,  0.12783333,  0.21095238,\n",
      "        0.14659341,  0.1899359 ,  0.13333333,  0.1899359 ,  0.15384615,\n",
      "        0.        ,  0.14007018,  0.14007018,  0.12783333,  0.15095238,\n",
      "        0.13333333,  0.10307692,  0.13333333,  0.10307692,  0.18833333]), 'params': [{'kbest__k': 15, 'lr__penalty': 'l1', 'lr__C': 1e-10}, {'kbest__k': 15, 'lr__penalty': 'l2', 'lr__C': 1e-10}, {'kbest__k': 15, 'lr__penalty': 'l1', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 15, 'lr__penalty': 'l2', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 15, 'lr__penalty': 'l1', 'lr__C': 1.0}, {'kbest__k': 15, 'lr__penalty': 'l2', 'lr__C': 1.0}, {'kbest__k': 15, 'lr__penalty': 'l1', 'lr__C': 100000.0}, {'kbest__k': 15, 'lr__penalty': 'l2', 'lr__C': 100000.0}, {'kbest__k': 15, 'lr__penalty': 'l1', 'lr__C': 10000000000.0}, {'kbest__k': 15, 'lr__penalty': 'l2', 'lr__C': 10000000000.0}, {'kbest__k': 16, 'lr__penalty': 'l1', 'lr__C': 1e-10}, {'kbest__k': 16, 'lr__penalty': 'l2', 'lr__C': 1e-10}, {'kbest__k': 16, 'lr__penalty': 'l1', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 16, 'lr__penalty': 'l2', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 16, 'lr__penalty': 'l1', 'lr__C': 1.0}, {'kbest__k': 16, 'lr__penalty': 'l2', 'lr__C': 1.0}, {'kbest__k': 16, 'lr__penalty': 'l1', 'lr__C': 100000.0}, {'kbest__k': 16, 'lr__penalty': 'l2', 'lr__C': 100000.0}, {'kbest__k': 16, 'lr__penalty': 'l1', 'lr__C': 10000000000.0}, {'kbest__k': 16, 'lr__penalty': 'l2', 'lr__C': 10000000000.0}, {'kbest__k': 17, 'lr__penalty': 'l1', 'lr__C': 1e-10}, {'kbest__k': 17, 'lr__penalty': 'l2', 'lr__C': 1e-10}, {'kbest__k': 17, 'lr__penalty': 'l1', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 17, 'lr__penalty': 'l2', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 17, 'lr__penalty': 'l1', 'lr__C': 1.0}, {'kbest__k': 17, 'lr__penalty': 'l2', 'lr__C': 1.0}, {'kbest__k': 17, 'lr__penalty': 'l1', 'lr__C': 100000.0}, {'kbest__k': 17, 'lr__penalty': 'l2', 'lr__C': 100000.0}, {'kbest__k': 17, 'lr__penalty': 'l1', 'lr__C': 10000000000.0}, {'kbest__k': 17, 'lr__penalty': 'l2', 'lr__C': 10000000000.0}, {'kbest__k': 18, 'lr__penalty': 'l1', 'lr__C': 1e-10}, {'kbest__k': 18, 'lr__penalty': 'l2', 'lr__C': 1e-10}, {'kbest__k': 18, 'lr__penalty': 'l1', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 18, 'lr__penalty': 'l2', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 18, 'lr__penalty': 'l1', 'lr__C': 1.0}, {'kbest__k': 18, 'lr__penalty': 'l2', 'lr__C': 1.0}, {'kbest__k': 18, 'lr__penalty': 'l1', 'lr__C': 100000.0}, {'kbest__k': 18, 'lr__penalty': 'l2', 'lr__C': 100000.0}, {'kbest__k': 18, 'lr__penalty': 'l1', 'lr__C': 10000000000.0}, {'kbest__k': 18, 'lr__penalty': 'l2', 'lr__C': 10000000000.0}, {'kbest__k': 19, 'lr__penalty': 'l1', 'lr__C': 1e-10}, {'kbest__k': 19, 'lr__penalty': 'l2', 'lr__C': 1e-10}, {'kbest__k': 19, 'lr__penalty': 'l1', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 19, 'lr__penalty': 'l2', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 19, 'lr__penalty': 'l1', 'lr__C': 1.0}, {'kbest__k': 19, 'lr__penalty': 'l2', 'lr__C': 1.0}, {'kbest__k': 19, 'lr__penalty': 'l1', 'lr__C': 100000.0}, {'kbest__k': 19, 'lr__penalty': 'l2', 'lr__C': 100000.0}, {'kbest__k': 19, 'lr__penalty': 'l1', 'lr__C': 10000000000.0}, {'kbest__k': 19, 'lr__penalty': 'l2', 'lr__C': 10000000000.0}, {'kbest__k': 20, 'lr__penalty': 'l1', 'lr__C': 1e-10}, {'kbest__k': 20, 'lr__penalty': 'l2', 'lr__C': 1e-10}, {'kbest__k': 20, 'lr__penalty': 'l1', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 20, 'lr__penalty': 'l2', 'lr__C': 1.0000000000000001e-05}, {'kbest__k': 20, 'lr__penalty': 'l1', 'lr__C': 1.0}, {'kbest__k': 20, 'lr__penalty': 'l2', 'lr__C': 1.0}, {'kbest__k': 20, 'lr__penalty': 'l1', 'lr__C': 100000.0}, {'kbest__k': 20, 'lr__penalty': 'l2', 'lr__C': 100000.0}, {'kbest__k': 20, 'lr__penalty': 'l1', 'lr__C': 10000000000.0}, {'kbest__k': 20, 'lr__penalty': 'l2', 'lr__C': 10000000000.0}], 'std_fit_time': array([  8.35071706e-04,   9.55804135e-04,   2.78426417e-03,\n",
      "         1.03409355e-03,   9.29169982e-03,   1.80928696e-03,\n",
      "         2.21022894e-03,   3.37785327e-04,   1.38757653e-03,\n",
      "         3.09506274e-04,   4.72237616e-04,   6.00257967e-04,\n",
      "         2.66920384e-03,   8.77976222e-04,   8.78622750e-03,\n",
      "         7.67189571e-04,   4.16565837e-03,   4.20501993e-04,\n",
      "         5.46761560e-03,   5.19472097e-04,   3.60447751e-05,\n",
      "         1.14022977e-04,   3.43156846e-03,   1.89930591e-04,\n",
      "         1.01576439e-02,   2.64233790e-04,   8.57332205e-03,\n",
      "         2.58838334e-04,   8.64861812e-03,   1.91856537e-04,\n",
      "         7.32069052e-04,   7.04911059e-04,   3.47210530e-04,\n",
      "         8.30459822e-05,   8.59953850e-03,   5.09178376e-04,\n",
      "         6.85274949e-03,   2.70474363e-04,   8.29695861e-03,\n",
      "         4.93746106e-04,   1.12873398e-04,   1.96644153e-04,\n",
      "         3.36140595e-03,   4.02200687e-04,   1.28004407e-02,\n",
      "         3.61645228e-04,   1.10333441e-02,   4.07157571e-04,\n",
      "         6.01790447e-03,   1.03523716e-03,   6.37840666e-04,\n",
      "         1.36010689e-04,   3.41045536e-03,   2.98237046e-04,\n",
      "         1.26341260e-02,   9.96938615e-04,   1.24431654e-03,\n",
      "         5.31605442e-04,   2.93532289e-03,   3.63807083e-04]), 'param_lr__penalty': masked_array(data = ['l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1'\n",
      " 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2'\n",
      " 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1'\n",
      " 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2'],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'split1_test_score': array([ 0.        ,  0.11111111,  0.11111111,  0.13333333,  0.2       ,\n",
      "        0.        ,  0.44444444,  0.16666667,  0.44444444,  0.        ,\n",
      "        0.        ,  0.11111111,  0.11111111,  0.15384615,  0.        ,\n",
      "        0.125     ,  0.25      ,  0.16666667,  0.44444444,  0.16666667,\n",
      "        0.        ,  0.11111111,  0.11111111,  0.        ,  0.22222222,\n",
      "        0.        ,  0.25      ,  0.16666667,  0.25      ,  0.14285714,\n",
      "        0.        ,  0.11111111,  0.10526316,  0.        ,  0.2       ,\n",
      "        0.        ,  0.25      ,  0.        ,  0.25      ,  0.        ,\n",
      "        0.        ,  0.10526316,  0.10526316,  0.        ,  0.18181818,\n",
      "        0.        ,  0.25      ,  0.        ,  0.25      ,  0.        ,\n",
      "        0.        ,  0.10526316,  0.10526316,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.16666667])}\n",
      "training time: 2.124s\n",
      "Best estimator found by grid search:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('kbest', SelectKBest(k=15, score_func=<function f_classif at 0x1a0755b7d0>)), ('lr', LogisticRegression(C=10000000000.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "7 (15.91%) positive labels in predictions\n",
      "[[35  4]\n",
      " [ 2  3]]\n",
      "TN: 35, FP: 4, FN: 2, TP: 3\n",
      "accuracy: 0.863636363636\n",
      "precision: 0.428571428571\n",
      "recall: 0.6\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6001f08e69fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Logistic Regression: SMOTE resampling\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_logistic_regression_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplot_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/antonrubisov/Documents/DAND/p7-ML/final_project/helper.py\u001b[0m in \u001b[0;36mplot_roc\u001b[0;34m(clf, X_test, y_test, X_train, y_train)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mlw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     plt.plot(fpr, tpr, color='darkorange',\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# SMOTE resampling\n",
    "X = df[features_list[1:]]\n",
    "y = df[features_list[0]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "print \"Logistic Regression: SMOTE resampling\"\n",
    "clf = run_logistic_regression_classifier(X_train, X_test, y_train, y_test)\n",
    "plot_roc(clf, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 features selected\n",
      "Logistic Regression: reduced features\n",
      "13 (13.00%) positive labels in training set\n",
      "5 (11.36%) positive labels in test set\n",
      "training time: 0.124s\n",
      "Best estimator found by grid search:\n",
      "LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "12 (27.27%) positive labels in predictions\n",
      "[[29 10]\n",
      " [ 3  2]]\n",
      "TN: 29, FP: 10, FN: 3, TP: 2\n",
      "accuracy: 0.704545454545\n",
      "precision: 0.166666666667\n",
      "recall: 0.4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-59193dac0685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Logistic Regression: reduced features\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_logistic_regression_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplot_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/antonrubisov/Documents/DAND/p7-ML/final_project/helper.pyc\u001b[0m in \u001b[0;36mplot_roc\u001b[0;34m(clf, X_test, y_test, X_train, y_train)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mlw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     plt.plot(fpr, tpr, color='darkorange',\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "selector = RFECV(clf, step=1)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "print(\"{} features selected\".format(selector.n_features_))\n",
    "\n",
    "X_train_reduced = selector.transform(X_train)\n",
    "X_test_reduced = selector.transform(X_test)\n",
    "\n",
    "print \"Logistic Regression: reduced features\"\n",
    "clf = run_logistic_regression_classifier(X_train_reduced, X_test_reduced, y_train, y_test)\n",
    "plot_roc(clf, X_test_reduced, y_test, X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('kbest', SelectKBest(k=15, score_func=<function f_classif at 0x114c5b7d0>)), ('lr', LogisticRegression(C=100000.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10000.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.81233\tPrecision: 0.36348\tRecall: 0.54250\tF1: 0.43531\tF2: 0.49386\n",
      "\tTotal predictions: 15000\tTrue positives: 1085\tFalse positives: 1900\tFalse negatives:  915\tTrue negatives: 11100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(n_splits = folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv.split(features, labels):\n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "\n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "\n",
    "my_dict = df.to_dict(orient=\"index\")      \n",
    "test_classifier(clf, my_dict, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5b312ea4d9ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Those values, however, will show that the second parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# is more influential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeature_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model\n",
    "# Those values, however, will show that the second parameter\n",
    "# is more influential\n",
    "feature_weights = (df[df.columns[1:]].std().as_matrix() * clf.coef_)[0]\n",
    "for idx, value in enumerate(feature_weights):\n",
    "    if abs(value) > 10:\n",
    "        print \"Feature[{}] {}: {:.2f}\".format(idx, df.columns[idx], value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature[0] bonus_to_salary_ratio: 1.92\n"
     ]
    }
   ],
   "source": [
    "feature_indices = [1 + idx for idx in list(clf.named_steps['kbest'].get_support(indices=True))]\n",
    "\n",
    "feature_weights = (df.iloc[:, feature_indices].std().as_matrix() * clf.named_steps['lr'].coef_)[0]\n",
    "for idx, value in enumerate(feature_weights):\n",
    "    if abs(value) > 0:\n",
    "        print \"Feature[{}] {}: {:.2f}\".format(idx, df.columns[idx+1], value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 (13.00%) positive labels in training set\n",
      "5 (11.36%) positive labels in test set\n",
      "training time: 0.258s\n",
      "Best estimator found by grid search:\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "18 (40.91%) positive labels in predictions\n",
      "[[24 15]\n",
      " [ 2  3]]\n",
      "TN: 24, FP: 15, FN: 2, TP: 3\n",
      "accuracy: 0.613636363636\n",
      "precision: 0.166666666667\n",
      "recall: 0.6\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.57487\tPrecision: 0.21960\tRecall: 0.85700\tF1: 0.34962\tF2: 0.54223\n",
      "\tTotal predictions: 15000\tTrue positives: 1714\tFalse positives: 6091\tFalse negatives:  286\tTrue negatives: 6909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, make_scorer\n",
    "\n",
    "def run_decision_tree_classifier(X_train, X_test, y_train, y_test, beta=5):\n",
    "    print \"{} ({:.2f}%) positive labels in training set\".format(len(y_train[y_train == True]), 100.0*len(y_train[y_train == True])/len(y_train))\n",
    "    print \"{} ({:.2f}%) positive labels in test set\".format(len(y_test[y_test == True]), 100.0*len(y_test[y_test == True])/len(y_test))\n",
    "\n",
    "    t0 = time()\n",
    "    param_grid = {\n",
    "             'min_samples_split': [2,5,10,15,20],\n",
    "             'max_depth': range(1,5,1),\n",
    "              }\n",
    "    f5_scorer = make_scorer(fbeta_score, beta=beta)\n",
    "    clf = GridSearchCV(estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
    "                       param_grid=param_grid,\n",
    "                       scoring=f5_scorer)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    print \"training time: {}s\".format(round(time() - t0, 3))\n",
    "    print \"Best estimator found by grid search:\"\n",
    "    print clf.best_estimator_\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print \"{} ({:.2f}%) positive labels in predictions\".format(len(y_pred[y_pred == True]), 100.0*len(y_pred[y_pred == True])/len(y_pred))\n",
    "\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    print conf\n",
    "    TN, FP, FN, TP = conf.ravel()\n",
    "    print \"TN: {}, FP: {}, FN: {}, TP: {}\".format(TN, FP, FN, TP)\n",
    "\n",
    "    # accuracy: what proportion was correctly predicted out of total population?\n",
    "    print \"accuracy: {}\".format((TP+TN)*1.0/(TN+FP+FN+TP))\n",
    "    # precision: of all the ones you predicted to be of class X, what ratio was correct?\n",
    "    print \"precision: {}\".format(TP*1.0/(TP+FP))\n",
    "    # recall: of all the ones that ARE of class X, what ratio did you successfully predict?\n",
    "    print \"recall: {}\".format(TP*1.0/(TP+FN))\n",
    "    \n",
    "    return clf.best_estimator_\n",
    "\n",
    "\n",
    "dt = run_decision_tree_classifier(X_train, X_test, y_train, y_test )\n",
    "test_classifier(dt, my_dict, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'other'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dt.feature_importances_)\n",
    "#11 13 17 19\n",
    "\n",
    "features_list[1:][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
